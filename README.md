# Tokenomics

> **Intelligent LLM Token Optimization Platform**  
> Reduce LLM costs by **90.7%** while maintaining response quality through smart caching, adaptive routing, and compression.

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)
[![Tested](https://img.shields.io/badge/tested-50%20queries-brightgreen.svg)]()

---

## Proven Results

We ran a comprehensive test with **50 diverse queries** across 7 categories. Here are the results:

| Metric | Value |
|--------|-------|
| **Cost Savings** | **90.7%** |
| Baseline Cost (50 queries) | $0.150788 |
| Optimized Cost (50 queries) | $0.013999 |
| **Total Saved** | **$0.136788** |
| Cache Hit Rate | 20% |
| Complexity Accuracy | 90% |

### Savings Breakdown by Component

```
Memory Layer (Cache):     $0.033  (22%)
Bandit Optimizer:         $0.103  (75%)
Compression:              $0.004  (3%)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOTAL SAVINGS:            $0.137  (90.7%)
```

ðŸ“„ **Full Test Results:** [COMPREHENSIVE_E2E_TEST_RESULTS.md](COMPREHENSIVE_E2E_TEST_RESULTS.md)

---

## How It Works

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        USER QUERY                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. MEMORY LAYER                                                  â”‚
â”‚     â”œâ”€â”€ Exact Cache: Hash-based O(1) lookup                       â”‚
â”‚     â”œâ”€â”€ Semantic Cache: Vector similarity search                  â”‚
â”‚     â””â”€â”€ Context Injection: Enrich prompts with similar responses  â”‚
â”‚                                                                   â”‚
â”‚     Result: 20% cache hit rate â†’ 0 tokens for cached queries      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. TOKEN ORCHESTRATOR                                            â”‚
â”‚     â”œâ”€â”€ Complexity Analysis: Simple/Medium/Complex (90% accuracy) â”‚
â”‚     â”œâ”€â”€ Knapsack Optimization: Maximize utility per token         â”‚
â”‚     â””â”€â”€ Token Budget Allocation: Smart distribution               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. BANDIT OPTIMIZER                                              â”‚
â”‚     â”œâ”€â”€ Strategy Selection: cheap/balanced/premium                â”‚
â”‚     â”œâ”€â”€ UCB Algorithm: Exploration vs exploitation                â”‚
â”‚     â””â”€â”€ Cost-Aware Routing: Balance quality and cost              â”‚
â”‚                                                                   â”‚
â”‚     Result: Learns to use cheaper models â†’ 75% of savings         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. COMPRESSION (LLMLingua)                                       â”‚
â”‚     â”œâ”€â”€ Long Query Detection: >500 chars or >150 tokens           â”‚
â”‚     â””â”€â”€ Intelligent Compression: Preserve meaning, reduce tokens  â”‚
â”‚                                                                   â”‚
â”‚     Result: 376 tokens saved on 5 long queries                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5. CASCADING INFERENCE                                           â”‚
â”‚     â”œâ”€â”€ Start with cheap model (gpt-4o-mini)                      â”‚
â”‚     â”œâ”€â”€ Quality check (threshold: 0.85)                           â”‚
â”‚     â””â”€â”€ Escalate to premium (gpt-4o) if needed                    â”‚
â”‚                                                                   â”‚
â”‚     Result: Quality maintained while minimizing cost              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      OPTIMIZED RESPONSE                           â”‚
â”‚                  90.7% cheaper, same quality                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Learning & Adaptation

Tokenomics gets smarter over time through multiple ML components:

### Pre-trained ML Models

| Model | Purpose | Accuracy |
|-------|---------|----------|
| **Complexity Classifier** | Categorize queries as simple/medium/complex | 90% |
| **Token Predictor** | Estimate response length before calling LLM | 72% |
| **Escalation Predictor** | Predict when quality escalation is needed | 85% |

### Continuous Learning

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BANDIT OPTIMIZER (UCB Algorithm)                               â”‚
â”‚                                                                 â”‚
â”‚  Learns from every query:                                       â”‚
â”‚  â”œâ”€â”€ Tracks cost vs quality for each strategy                   â”‚
â”‚  â”œâ”€â”€ Balances exploration (try new) vs exploitation (use best)  â”‚
â”‚  â””â”€â”€ Adapts routing based on actual outcomes                    â”‚
â”‚                                                                 â”‚
â”‚  Result: Routing improves automatically with usage              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Collection Pipeline

The platform collects training data from real queries to retrain models:

```bash
# Collect training data
python scripts/collect_training_data.py

# Retrain models with new data
python scripts/train_ml_models.py

# Evaluate model performance
python scripts/evaluate_ml_models.py
```

**Pre-trained models:** `models/` directory contains ready-to-use `.pkl` files.

---

## Quick Start

### 1. Clone and Setup

```bash
git clone https://github.com/yourusername/tokenomics.git
cd tokenomics
python3 -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt
pip install -e .
```

### 2. Configure

```bash
cp env.template .env
# Edit .env and add your OPENAI_API_KEY
```

### 3. Run

```python
from tokenomics.core import TokenomicsPlatform
from tokenomics.config import TokenomicsConfig
import os
from dotenv import load_dotenv

load_dotenv()

config = TokenomicsConfig.from_env()
platform = TokenomicsPlatform(config=config)

# Run optimized query
result = platform.query("What is machine learning?")
print(f"Response: {result['response']}")
print(f"Tokens: {result['tokens_used']}")
print(f"Cache hit: {result['cache_hit']}")
```

### 4. Try the Playground

```bash
python app.py
# Open http://localhost:5000/playground
```

---

## Components

| Component | Purpose | Key Metric |
|-----------|---------|------------|
| **Memory Layer** | Exact + semantic caching | 20% cache hit rate |
| **Token Orchestrator** | Complexity analysis & budget allocation | 90% accuracy |
| **Bandit Optimizer** | Cost-aware model routing | 75% of savings |
| **LLMLingua Compression** | Reduce long query tokens | 376 tokens saved |
| **Cascading Inference** | Quality-protected model selection | Maintains quality |
| **Token Predictor** | ML-based response length prediction | 72% accuracy |

---

## Test Categories

Our comprehensive test included:

| Category | Count | Purpose | Result |
|----------|-------|---------|--------|
| Simple queries | 10 | Test cheap strategy | âœ… Low cost |
| Medium queries | 10 | Test balanced strategy | âœ… Appropriate routing |
| Complex queries | 10 | Test premium + cascading | âœ… Quality maintained |
| Exact duplicates | 5 | Test exact cache | âœ… **100% hit rate** |
| Semantic variations | 5 | Test semantic cache | âœ… 40% hit rate |
| Long queries (>500 chars) | 5 | Test compression | âœ… All compressed |
| Mixed scenarios | 5 | Edge cases | âœ… Handled correctly |

---

## Project Structure

```
tokenomics/
â”œâ”€â”€ tokenomics/              # Core platform
â”‚   â”œâ”€â”€ core.py              # Main entry point
â”‚   â”œâ”€â”€ memory/              # Caching layer
â”‚   â”œâ”€â”€ orchestrator/        # Token allocation
â”‚   â”œâ”€â”€ bandit/              # Strategy selection
â”‚   â”œâ”€â”€ compression/         # LLMLingua integration
â”‚   â””â”€â”€ ml/                  # Token prediction
â”œâ”€â”€ templates/               # Web UI
â”œâ”€â”€ static/                  # CSS/JS
â”œâ”€â”€ app.py                   # Flask server
â”œâ”€â”€ scripts/                 # Utility scripts
â”œâ”€â”€ tests/                   # Test suite
â”œâ”€â”€ docs/                    # Documentation
â””â”€â”€ examples/                # Usage examples
```

---

## Documentation

- **[Architecture](docs/architecture/overview.md)** - System design
- **[Installation](docs/guides/installation.md)** - Detailed setup
- **[Configuration](docs/guides/configuration.md)** - All options
- **[Test Results](COMPREHENSIVE_E2E_TEST_RESULTS.md)** - Full proof of work

---

## API Reference

### `platform.query()`

```python
result = platform.query(
    query="Your question here",
    token_budget=4000,          # Optional: max tokens
    use_cache=True,             # Use memory layer
    use_bandit=True,            # Use strategy selection
    use_compression=True,       # Compress long queries
    use_cost_aware_routing=True # Optimize for cost
)
```

**Returns:**
```python
{
    "response": "...",           # LLM response
    "tokens_used": 250,          # Total tokens
    "cache_hit": False,          # Was this cached?
    "strategy": "cheap",         # Strategy used
    "model": "gpt-4o-mini",      # Model used
    "latency_ms": 1234,          # Response time
}
```

---

## License

MIT License - see [LICENSE](LICENSE)

---

## Acknowledgments

- [RouterBench](https://github.com/withmartian/routerbench) - Routing methodology
- [LLMLingua](https://github.com/microsoft/LLMLingua) - Compression
- [FAISS](https://github.com/facebookresearch/faiss) - Vector search

---

**Built for the LLM optimization community**
