# Tokenomics Cost Benchmark Results

**Generated:** 2025-12-31 04:37:23  
**Git Commit:** `88aea27`

---

## Benchmark Configuration

| Setting | Value |
|---------|-------|
| Baseline Model | `gpt-4o` |
| Tokenomics Default Model | `gpt-4o-mini` |
| Tokenomics Premium Model | `gpt-4o` |
| Temperature | 0.3 (baseline) / varies (tokenomics) |
| Max Tokens | 512 (baseline) / varies (tokenomics) |
| Cache | Disabled (baseline) / Enabled (tokenomics) |
| Compression | Disabled (baseline) / Enabled (tokenomics) |
| Routing | Disabled (baseline) / Bandit UCB (tokenomics) |

---

## Workload Summary

| Category | Count |
|----------|-------|
| Simple | 3 |
| Medium | 4 |
| Complex | 3 |
| **Total** | **10** |

---

## Aggregate Summary

### Cost & Token Savings

| Metric | Value |
|--------|-------|
| Total Baseline Tokens | 3,963 |
| Total Tokenomics Tokens | 3,040 |
| **Mean Token Savings** | **14.5%** |
| Median Token Savings | 8.6% |
| Total Baseline Cost | $0.037800 |
| Total Tokenomics Cost | $0.001836 |
| **Total Cost Savings** | **$0.035964** |
| **Mean Cost Savings** | **85.5%** |
| Median Cost Savings | 94.5% |

### Cache Performance

| Metric | Value |
|--------|-------|
| Cache Hit Rate | 0.0% |
| Exact Cache Hits | 0 |
| Semantic Cache Hits | 0 |
| Cache Misses | 10 |

### Routing Distribution

| Strategy | Count | Percentage |
|----------|-------|------------|
| Cheap (gpt-4o-mini) | 3 | 30.0% |
| Balanced (gpt-4o-mini) | 2 | 20.0% |
| Premium (gpt-4o) | 5 | 50.0% |

### Quality & Reliability

| Metric | Value |
|--------|-------|
| Successful Prompts | 10/10 |
| Quality Failures | 0 |
| Mean Latency Delta | 157ms |

---

## Per-Prompt Results

| ID | Category | Baseline Tokens | Tokenomics Tokens | Token Savings | Baseline Cost | Tokenomics Cost | Cost Savings | Cache | Strategy |
|----|----------|-----------------|-------------------|---------------|---------------|-----------------|--------------|-------|----------|
| simple-1 | simple | 21 | 21 | 0.0% | $0.000105 | $0.000006 | 94.0% | miss | premium |
| simple-2 | simple | 25 | 25 | 0.0% | $0.000130 | $0.000130 | 0.0% | miss | cheap |
| simple-3 | simple | 24 | 24 | 0.0% | $0.000150 | $0.000009 | 94.0% | miss | premium |
| medium-1 | medium | 477 | 549 | -15.1% | $0.004635 | $0.000321 | 93.1% | miss | cheap |
| medium-2 | medium | 536 | 455 | 15.1% | $0.005195 | $0.000263 | 94.9% | miss | balanced |
| medium-3 | medium | 523 | 324 | 38.0% | $0.005050 | $0.000184 | 96.4% | miss | balanced |
| medium-4 | medium | 330 | 323 | 2.1% | $0.003180 | $0.000187 | 94.1% | miss | cheap |
| complex-1 | complex | 665 | 382 | 42.6% | $0.006320 | $0.000209 | 96.7% | miss | premium |
| complex-2 | complex | 635 | 443 | 30.2% | $0.006087 | $0.000250 | 95.9% | miss | premium |
| complex-3 | complex | 727 | 494 | 32.0% | $0.006948 | $0.000277 | 96.0% | miss | premium |

---

## Caveats

1. **Baseline Configuration**: Baseline runs with cache completely disabled (no read, no write). This ensures baseline costs are not artificially reduced by prior tokenomics runs.

2. **Cache Cold Start**: Tokenomics cache starts empty at benchmark start. The cache is NOT cleared between prompts, allowing natural cache warming.

3. **Workload Dependence**: Results are specific to this workload (10 prompts). Production savings depend on actual query patterns, repetition, and complexity distribution.

4. **Quality Metric**: Quality is measured using `minlen` check. This is a proxy metric, not a comprehensive quality evaluation.

5. **Latency Variance**: Latency measurements include API round-trip time and may vary based on network conditions and API load.

6. **Model Pricing**: Costs are estimated based on OpenAI's published pricing as of the benchmark date. Actual costs may vary.

---

## Reproduction Steps

```bash
# From repository root
cd /path/to/Tokenomics

# Ensure .env has OPENAI_API_KEY set
source venv/bin/activate

# Run benchmark
python scripts/run_cost_benchmark.py \
    --workload benchmarks/workloads_v0.json \
    --output BENCHMARK_COST_RESULTS.md \
    --quality_check minlen \
    --seed 42
```

Expected runtime: ~50 seconds (varies with API latency)

---

*Report generated by `scripts/run_cost_benchmark.py`*
